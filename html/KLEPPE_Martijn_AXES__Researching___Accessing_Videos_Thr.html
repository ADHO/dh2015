<?xml version="1.0" encoding="UTF-8"?>
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)-->
    <title>AXES: Researching &amp; Accessing Videos Through Multimodal Analyses</title>
    <meta name="author" content="Martijn Kleppe Erasmus University Rotterdam, Netherlands, The , Max Kemman Erasmus University Rotterdam, Netherlands, The , Peggy Van der Kreeft Deutsche Welle , Kay Macquarrie Deutsche Welle , and Kevin McGuinness Dublin City University"/>
    <meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"/>
    <meta name="DC.Title" content="AXES: Researching &amp; Accessing Videos Through Multimodal Analyses"/>
    <meta name="DC.Type" content="Text"/>
    <meta name="DC.Format" content="text/html"/>
    <link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"/>
  </head>
  <body class="simple" id="TOP">
    <div class="stdheader autogenerated">
      <h1 class="maintitle">AXES: Researching &amp; Accessing Videos Through Multimodal Analyses</h1>
    </div>
    <div class="dhconvalidator-xml-link">
      <a href="KLEPPE_Martijn_AXES__Researching___Accessing_Videos_Through_Multimodal_Analyses.xml">XML</a>
    </div>
    <p>AXES—Access for Audiovisual Archives is a research project developing tools for new and engaging ways to interact with audiovisual libraries, integrating advanced audio and video analysis technologies. This poster and demo will showcase the system that is developed for academic researchers and journalists. The tool allows them to search and retrieve video segments through metadata and audio analysis, as well as visual concepts and similarity searches. </p>
    <p>Background</p>
    <p>In the near future, audiovisual material is perhaps the biggest wave of data that will become available for academic researchers (Smith, 2013). This type of material has a big value for the digital humanities as it is multilayered: a single document can provide information regarding language, emotions, speech acts, narrative plots, and references to people, places, and events. This richness provides interesting data for various disciplines and holds the promise of multidisciplinary collaboration between, e.g., computer sciences, social sciences, and the humanities (Ordelman et al., 2014). </p>
    <p>However, the use of audiovisual data by scholars in the humanities and the application of digital methods for analysis are still in their infancy for several reasons. One of them is the lack of useful systems that help academic researchers search through the large amounts of audiovisual data (De Jong et al., 2011). Given the multimodal nature of audiovisual data, different types of techniques are required to provide access to the data. The overall aim of the research project AXES—Access for Audiovisual Archives is to develop tools that allow novel ways of using digital audiovisual libraries, helping users to discover, browse, navigate, search, and enrich archives. 
      <sup>1</sup> Within the project, three systems are developed. In this poster and demonstration we will show the AXES RESEARCH system that was developed to cater the needs of humanities scholars and journalists.
    </p>
    <p>
      <span style="font-weight:bold">AXES RESEARCH System </span>
    </p>
    <p>Building on several requirements studies amongst humanities scholars (Kemman et al., 2014; 2013a; 2012) and journalists (Kemman et al., 2013b), the AXES RESEARCH system is an advanced search and retrieval system that combines various technologies from computer vision such as face, object, and place recognition; similarity searching; and automatic speech recognition, making it easier for the user to find relevant material, not dependent on available metadata (Van der Kreeft et al., 2014). The strength of the system is a combination of different technologies working in the background. </p>
    <div class="figure">
      <img src="Pictures/image1.png" alt="" class="inline" style=" width:16.002cm; height:10.17763888888889cm;"/>
    </div>
    <p>Figure 1. The AXES RESEARCH start interface provides users with a simple search interface and shows their recently viewed videos and queries. </p>
    <p>The following key search technologies are used in the AXES RESEARCH system: text/spoken words search, visual search, and similarity search. </p>
    <p>Text Search / Spoken Words Search</p>
    <p>All metadata of the audiovisual programs and its spoken words are stored and indexed. Spoken words are provided in the form of a transcript originally provided with the audiovisual data or are automatically produced by Automatic Speech Recognition (ASR). </p>
    <p>Visual Search </p>
    <p>The system uses text-based queries to look for visual objects. This is done in conjunction with an external search engine and uses on-the-fly methods (Parkhi et al., 2012). If a user makes a text search for ‘Brandenburg gate’, the query is sent to a search engine like Google or Bing that produces a sample of the top-n images. From the results, a model is created and used to detect similar objects in the archive. </p>
    <div class="figure">
      <img src="Pictures/image2.png" alt="" class="inline" style=" width:16.002cm; height:8.678333333333333cm;"/>
    </div>
    <p>Figure 2. AXES RESEARCH thumbnail view of visual search results. Results can also be viewed in detailed view. </p>
    <p>Generally, the system supports three types of visual search: visual categories (Parkhi et al., 2012), faces (Simonyan et al., 2010), and specific places or logos (Fernando et al., 2013). Furthermore, the user can also search for events. The system recognizes events based on multimodal input, including audio and visual features (Revaud et al., 2013). </p>
    <p>Similarity Search </p>
    <p>Instead of entering keywords, a search can also be based on internal or external images, also known as content-based image search (Smeulders, 2000). A similarity search can be done by using one or more images, either a keyframe shown from returned results, or an image uploaded by the user, comparable with the query by image technique as implemented at Google Images. 
      <sup>2</sup>
    </p>
    <p>
      <span style="font-weight:bold">Results: User Testing </span>
    </p>
    <p>A total of 78 participants were involved in the evaluation sessions of AXES RESEARCH. Overall, participants were very interested in the system that assists them in research practices. In general, the look and feel of the prototype was appreciated, and users concluded that the functionalities of integrating video and audio, including similarity search, worked. User input and suggestions for enhancement served to improve the coming versions of the AXES system that will focus on home users. </p>
    <p>Conclusion </p>
    <p>AXES RESEARCH offers academics a novel way of exploring audiovisual content. They can take advantage of a powerful system, without the need to be involved in all the technical intricacies, allowing them to incorporate audiovisual materials in their research practice, which is currently rarely done given the absence of a system like AXES RESEARCH that helps them search through large amount of audiovisual data. </p>
    <p>Acknowledgment </p>
    <p>This work is supported by the EU FP7 programme as EU Project FP7 AXES ICT-269980. </p>
    <p>Notes</p>
    <p>1. www.axes-project.eu.</p>
    <p>2. www.images.google.com.</p>
    <div class="bibliogr" id="index.xml-back.1_div.1">
      <h2>
        <span class="headingNumber">Appendix A </span>
      </h2>
      <div class="listhead">Bibliography</div>
      <ol class="listBibl">
        <li id="index.xml-bibl-w1009353aab3b3b1b1b3">
          <div class="biblfree">
            <span style="font-weight:bold">De Jong, F., Ordelman, R. and Scagliola, S.</span> (2011). Audio-Visual Collections and the User Needs of Scholars in the Humanities: A Case for Co-Development. In 
            <span style="font-style:italic">Proceedings of the 2nd Conference on Supporting Digital Humanities (SDH 2011)</span>, Copenhagen, Denmark, 17–18 November 2011.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1b5">
          <div class="biblfree">
            <span style="font-weight:bold">Fernando, B. and Tuytelaars, T.</span> (2013). Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-Level Representation. 
            <span style="font-style:italic">ICCV</span>, Sydney, Australia, 3–6 December 2013.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1b7">
          <div class="biblfree">
            <span style="font-weight:bold">Kemman, M., Kleppe, M., Beunders, H.</span> (2012). Who Are the Users of a Video Search System? Classifying a Heterogeneous Group with a Profile Matrix. In 
            <span style="font-style:italic">13th International Workshop on Image Analysis for Multimedia Interactive Services</span>, Dublin, Ireland, 23–25 May 2012.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1b9">
          <div class="biblfree">
            <span style="font-weight:bold">Kemman, M., Kleppe, M. and Maarseveen, J.</span> (2013a). Eye Tracking the Use of a Collapsible Facets Panel in a Search Interface. In 
            <span style="font-style:italic">Research and Advanced Technology for Digital Libraries</span>. Berlin: Springer, pp. 405–8, doi:10.1007/978-3-642-40501-3_47.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c11">
          <div class="biblfree">
            <span style="font-weight:bold">Kemman, M., Kleppe, M., Nieman, B. and Beunders, H.</span> (2013b). Dutch Journalism in the Digital Age [El Periodismo Holandés En La Era Digital]. 
            <span style="font-style:italic">Icono 14, </span>
            <span style="font-weight:bold">11</span>(2): 163–81, doi:10.7195/ri14.v11i2.596.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c13">
          <div class="biblfree">
            <span style="font-weight:bold">Kemman, M., Kleppe, M. and Scagliola, S.</span> (2014). Just Google It. In Mills, C., Pidd, M. and Ward, E. (eds), 
            <span style="font-style:italic">Proceedings of the Digital Humanities Congress 2012</span>. Sheffield, UK: HRI Online Publications.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c15">
          <div class="biblfree">
            <span style="font-weight:bold">Ordelman, R., Kemman, M., Kleppe, M. and De Jong, F.</span> (2014). Sound and (Moving Images) in Focus: How to Integrate Audiovisual Material in Digital Humanities Research. 
            <span style="font-style:italic">Digital Humanities 2014</span>, 
            <span style="color:2E2E2E">Lausanne, Switzerland, 6–12 July 2014,</span> http://dharchive.org/paper/DH2014/Workshops-914.xml.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c17">
          <div class="biblfree">
            <span style="font-weight:bold">Parkhi, O. M. et al.</span> (2012). “On-the-Fly Specific Person Retrieval.” In 
            <span style="font-style:italic">13th International Workshop on Image Analysis for Multimedia Interactive Services</span>, Dublin, Ireland, 23–25 May 2012.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c19">
          <div class="biblfree">
            <span style="font-weight:bold">Revaud, J., Douze, M., Schmid, C. and Jégou, H.</span> (2013). Event Retrieval in Large Video Collections with Circulant Temporal Encoding. 
            <span style="font-style:italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2013, Portland, OR.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c21">
          <div class="biblfree">
            <span style="font-weight:bold">Simonyan, K., Parkhi, O. M., Vedaldi, A. and Zisserman, A.</span> (2013). Fisher Vector Faces in the Wild. British Machine Vision Conference, Bristol, UK, 9–13 September 2013.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c23">
          <div class="biblfree">
            <span style="font-weight:bold">Smeulders, A. W., Worring, M., Santini, S., Gupta, A. and Jain, R.</span> (2000). Content-Based Image Retrieval at the End of the Early Years: Pattern Analysis and Machine Intelligence. 
            <span style="font-style:italic">IEEE Transactions,</span>
            <span style="font-weight:bold">22</span>(12): 1349–80.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c25">
          <div class="biblfree">
            <span style="font-weight:bold">Smith, J. R.</span> (2013). Riding the Multimedia Big Data Wave. In 
            <span style="font-style:italic">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval—SIGIR ’13</span>. New York: ACM Press, doi:10.1145/2484028.2494492.
          </div>
        </li>
        <li id="index.xml-bibl-w1009353aab3b3b1b1c27">
          <div class="biblfree">
            <span style="font-weight:bold">Van der Kreeft, P., Macquarrie, K., Kemman, M., Kleppe, M. and McGuinness, K.</span> (2014). AXES-RESEARCH: A User-Oriented Tool for Enhanced Multimodal Search and Retrieval in Audiovisual Libraries. In 
            <span style="font-style:italic">2014 12th International Workshop on Content-Based Multimedia Indexing (CBMI)</span>, IEEE, Klagenfurt, Austria, 18–20 June 2014, 1–4, doi:10.1109/CBMI.2014.6849852.
          </div>
        </li>
      </ol>
    </div>
    <div class="stdfooter autogenerated">
      <address>Martijn Kleppe (kleppe@eshcc.eur.nl), Erasmus University Rotterdam, Netherlands, The and Max Kemman (max.kemman@uni.lu), Erasmus University Rotterdam, Netherlands, The and Peggy Van der Kreeft (peggy.van-der-kreeft@dw.de), Deutsche Welle and Kay Macquarrie (kay.macquarrie@dw.de), Deutsche Welle and Kevin McGuinness (kevin.mcguinness@dcu.ie), Dublin City University</address>
    </div>
  </body>
</html>
